version: '3.8'

services:
  # Main Omnix service - runs all services in one container
  omnix:
    build: .
    container_name: omnix
    ports:
      - "5000:5000"    # Main Flask app
      - "8000:8000"    # STT server (Parakeet)
      - "8001:8001"    # Realtime WebSocket server
      - "8020:8020"    # TTS server (Chatterbox)
    volumes:
      - ./data:/app/data
      - ./voice_clones:/app/voice_clones
      - ./models/llm:/app/models/llm
      - ~/.cache/huggingface:/root/.cache/huggingface  # Cache HuggingFace models
    environment:
      - PYTHONUNBUFFERED=1
      - FLASK_APP=app.py
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    # Use a startup script to run all services
    command: >
      bash -c "
        echo 'Starting Parakeet STT Server...' &&
        python parakeet_stt_server.py &
        sleep 10 &&
        echo 'Starting Chatterbox TTS Server...' &&
        python chatterbox_tts_server.py &
        sleep 5 &&
        echo 'Starting Realtime Server...' &&
        python realtime_server.py &
        sleep 3 &&
        echo 'Starting Main App...' &&
        python app.py
      "

  # llama.cpp server for local LLM inference
  # Place your GGUF model file in models/llm/ folder and set LLAMA_MODEL env var
  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: llama-cpp
    ports:
      - "8080:8080"
    volumes:
      - ./models/llm:/models
    environment:
      - HOST=0.0.0.0
      - PORT=8080
      # Default model: Mistral-7B GGUF (auto-loaded if present)
      - LLAMA_MODEL=${LLAMA_MODEL:-mistral-7b-instruct-v0.2.Q4_K_M.gguf}
    # Default context size and GPU layers
    command: >
      sh -c "
        if [ -n \"$$LLAMA_MODEL\" ]; then
          echo \"Loading model: $$LLAMA_MODEL\";
          /usr/bin/llama-server -m /models/$$LLAMA_MODEL -c 512 -ngl 33 --host 0.0.0.0 --port 8080;
        else
          echo 'No model specified. Place GGUF files in models/llm/ and set LLAMA_MODEL environment variable.';
          echo 'Example: docker compose up -e LLAMA_MODEL=mistral-7b-instruct-v0.2.Q4_K_M.gguf';
          # Keep container running for healthcheck
          tail -f /dev/null;
        fi
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: Separate TTS service (Chatterbox)
  # Uncomment if running TTS as separate container
  # chatterbox-tts:
  #   build: .
  #   container_name: chatterbox-tts
  #   ports:
  #     - "8020:8020"
  #   volumes:
  #     - ./voice_clones:/app/voice_clones
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   command: python chatterbox_tts_server.py
  #   restart: unless-stopped

  # Optional: Separate STT service (Parakeet)
  # Uncomment if running STT as separate container
  # parakeet-stt:
  #   build: .
  #   container_name: parakeet-stt
  #   ports:
  #     - "8000:8000"
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   command: python parakeet_stt_server.py
  #   restart: unless-stopped

# Named volumes for persistence
volumes:
  huggingface-cache: