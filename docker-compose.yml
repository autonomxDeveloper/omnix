version: '3.8'

services:
  # Main Omnix service - runs all services in one container
  omnix:
    image: omnix:latest
    build: .
    container_name: omnix
    ports:
      - "5000:5000"    # Main Flask app
      - "8000:8000"    # STT server (Parakeet)
      - "8001:8001"    # Realtime WebSocket server
      - "8020:8020"    # TTS server (Chatterbox)
      - "8080:8080"    # llama.cpp server
    volumes:
      - ./data:/app/data
      - ./voice_clones:/app/voice_clones
      - ./models/llm:/app/models/llm
      - ./models/server:/app/models/server
      - ~/.cache/huggingface:/root/.cache/huggingface  # Cache HuggingFace models
    environment:
      - PYTHONUNBUFFERED=1
      - FLASK_APP=app.py
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    # Use a startup script to run all services
    command: >
      bash -c "
        echo 'Starting Parakeet STT Server...' &&
        python parakeet_stt_server.py &
        sleep 30 &&
        echo 'Starting Chatterbox TTS Server...' &&
        python chatterbox_tts_server.py &
        sleep 15 &&
        echo 'Waiting for TTS model to fully load...' &&
        sleep 10 &&
        echo 'Starting Realtime Server...' &&
        python realtime_server.py &
        sleep 3 &&
        echo 'Starting llama.cpp Server...' &&
        chmod +x /app/start_llama_server.sh &&
        /app/start_llama_server.sh &
        sleep 5 &&
        echo 'Starting Main App...' &&
        python app.py
      "

  # Note: llama.cpp server removed - use local installation or run separately
  # To enable LLM: download GGUF model to models/llm/ and run llama.cpp locally
  # Optional: Separate TTS service (Chatterbox)
  # Uncomment if running TTS as separate container
  # chatterbox-tts:
  #   build: .
  #   container_name: chatterbox-tts
  #   ports:
  #     - "8020:8020"
  #   volumes:
  #     - ./voice_clones:/app/voice_clones
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   command: python chatterbox_tts_server.py
  #   restart: unless-stopped

  # Optional: Separate STT service (Parakeet)
  # Uncomment if running STT as separate container
  # parakeet-stt:
  #   build: .
  #   container_name: parakeet-stt
  #   ports:
  #     - "8000:8000"
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   command: python parakeet_stt_server.py
  #   restart: unless-stopped

# Named volumes for persistence
volumes:
  huggingface-cache: